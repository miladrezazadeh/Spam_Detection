# -*- coding: utf-8 -*-
"""Spam_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LKya4w9vQrzCiPdF06aioB-vkFLDs6pp
"""

from google.colab import drive
drive.mount('/content/drive')

## Import required libraries

import re
import nltk
import pandas as pd
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from nltk.stem import WordNetLemmatizer

nltk.download('stopwords')

"""## Using Stemming, Bag of Words and Naive Bayes"""

## Load the dataset

messages = pd.read_csv('/content/drive/MyDrive/NLP/Spam_Detection/SMSSpamCollection', sep='\t',
                           names=["label", "message"])

messages.head()

messages.shape

## Data cleaning and preprocessing

ps = PorterStemmer()
corpus = []
for i in range(0, len(messages)):
    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])
    review = review.lower()
    review = review.split()
    
    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

corpus

# Creating vectorized representation via Bag of Words

cv = CountVectorizer(max_features=2500)
X = cv.fit_transform(corpus).toarray()

X

X.shape

## Convert your label to dummy 

y=pd.get_dummies(messages['label'])
y=y.iloc[:,1].values

y

y.shape

## Train test split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)

X_train.shape

y_train.shape

## Implementing Niave Bayes as a model
spam_detect_model = MultinomialNB().fit(X_train, y_train)

## Test accuracy
y_pred=spam_detect_model.predict(X_test)

confusion_m = confusion_matrix(y_test, y_pred)

confusion_m

accuracy = accuracy_score(y_test,y_pred)

accuracy

"""## Using lemmatization, TF-IDF and Naive Bayes"""

nltk.download('wordnet')

texts = pd.read_csv('/content/drive/MyDrive/NLP/Spam_Detection/SMSSpamCollection', sep='\t',
                           names=["label", "message"])

texts.head()

## Data Cleaning and preprocessing

wordnet = WordNetLemmatizer()
corpus = []
for i in range(0, len(texts)):
    review = re.sub('[^a-zA-Z]', ' ', texts['message'][i])
    review = review.lower()
    review = review.split()
    
    review = [wordnet.lemmatize(word) for word in review if not word in stopwords.words('english')]
    review = ' '.join(review)
    corpus.append(review)

# Creating the TF-IDF model
from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer(max_features=2500)
X_tf = cv.fit_transform(corpus).toarray()

X_tf.shape

y_tf = pd.get_dummies(texts['label'])
y_tf=y_tf.iloc[:,1].values

y_tf.shape

## Train test split

X_train, X_test, y_train, y_test = train_test_split(X_tf, y_tf, test_size = 0.20, random_state = 0)

## Implementing Niave Bayes as a model
spam_detect_model = MultinomialNB().fit(X_train, y_train)

## Test accuracy
y_pred=spam_detect_model.predict(X_test)

confusion_m = confusion_matrix(y_test, y_pred)

confusion_m

accuracy = accuracy_score(y_test,y_pred)

accuracy

